{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ed966ba",
   "metadata": {},
   "source": [
    "# Sentiment Analysis deployed in AWS - Using SVM, Logistic Regression, and Bert for on-demand predictions\n",
    "\n",
    "The goal of this project is to compare the power of **SVM**, **Logistic Regression**, and **BERT** in a text classification task as simple as Tweet Sentiment Analysis Classification. \n",
    "\n",
    "\n",
    "### What we know about the data\n",
    "\n",
    "The data was extracted from a [Sentiment140 dataset with 1.6 million tweets](https://www.kaggle.com/datasets/kazanova/sentiment140/code). It contains the following features\n",
    "\n",
    "- target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "- ids: The id of the tweet ( 2087)\n",
    "- date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "- flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "- user: the user that tweeted (robotickilldozr)\n",
    "- text: the text of the tweet (Lyx is cool)\n",
    "\n",
    "To keep it simple we will only focus on 'target' and 'text' features for text classification.\n",
    "\n",
    "### Data Pre-processing\n",
    "\n",
    "In this notebook we will process and transform Tweet text data by using the following data transformation techniques:\n",
    "\n",
    "- Tokenization\n",
    "- Lemmatization\n",
    "- Stop Words Removal\n",
    "- Contractions conversion\n",
    "\n",
    "### Text Representation\n",
    "\n",
    "For this project we will initially use Tf-Idf vectorization for feature engineering. In the future we might perform techniques such as Bag of Words or Word Embeddings via Word2Vec and compare scores.\n",
    "\n",
    "\n",
    "### Validation and Evaluation\n",
    "\n",
    "I will use k-fold cross-validation to perform multiple trainings and use AUC as an evaluation metric for hyperparameyter tuning.\n",
    "\n",
    "### Deployment into production with AWS Sagemaker Hosting and Visualization with AWS QuickSight\n",
    "\n",
    "The following AWS services will be used to deploy an API endpoint with Data Visualizationa and on-demand predictions:\n",
    "\n",
    "AWS Sagemaker Hosting will deploy my ML models. For this, I will be using Docker to containerize my built inference models and deploy them in Sagemaker. I will be using A/B release strategy to load balance the traffic to different hosted inferences.\n",
    "\n",
    "**AWS API Gateway** will be used to create an API endpoing where the request will return the respopnse of the inference model. \n",
    "\n",
    "**AWS Lambda** will take care of data transformation and preprocessing as well as evoking the Sagemaker inference. \n",
    "\n",
    "Requests will be stored in **AWS Aurora Servelress SQL** Database. \n",
    "\n",
    "**AWS QuickSight** will be used for data vizualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7c0f4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pltz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05df2507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception reporting mode: Minimal\n"
     ]
    }
   ],
   "source": [
    "%xmode Minimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c1e4d23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target         ids                          date      flag  \\\n",
       "0             0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1             0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2             0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3             0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4             0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...         ...         ...                           ...       ...   \n",
       "1599995       4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996       4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997       4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998       4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999       4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATASET_COLUMNS = ['target','ids','date','flag','user','text']\n",
    "DATASET_ENCODING = \"ISO-8859-1\"\n",
    "df = pd.read_csv('tweets.csv', encoding=DATASET_ENCODING, names=DATASET_COLUMNS)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5e03b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   target  1600000 non-null  int64 \n",
      " 1   ids     1600000 non-null  int64 \n",
      " 2   date    1600000 non-null  object\n",
      " 3   flag    1600000 non-null  object\n",
      " 4   user    1600000 non-null  object\n",
      " 5   text    1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40525b36",
   "metadata": {},
   "source": [
    "### Class balance - Too good to be true..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54a36052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    800000\n",
       "4    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c3690d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@deviousrex Only if they try to take the moonshine bottles I will have strapped around my waste for race hydration!  #running\n"
     ]
    }
   ],
   "source": [
    "data = df[['text', 'target']].sample(n=100000).reset_index(drop=True)\n",
    "print(data['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "68016010",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    50023\n",
       "4    49977\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1a9bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string, contractions\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import re\n",
    "import  string\n",
    "\n",
    "\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18d976c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import contractions\n",
    "import re\n",
    "\n",
    "tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Step 1: Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "   # Step 2: Remove URLs starting with http or https\n",
    "    text = re.sub(r'http[s]?://[^\\s]+', '', text)\n",
    "    \n",
    "    # Step 3: Remove URLs with common top-level domains (TLDs)\n",
    "    text = re.sub(r'(?:www\\.)?[a-zA-Z0-9-]+\\.(com|org|net|edu|gov|io)[^\\s]*', '', text)\n",
    "    \n",
    "    # Step 4: Remove www. prefixes not covered by previous steps\n",
    "    text = re.sub(r'www\\.[a-zA-Z0-9-]+\\.[^\\s]+', '', text)\n",
    "    \n",
    "    # Step 5: Remove mentions\n",
    "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
    "    \n",
    "    # Step 6: Remove all punctuation\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Step 7: Remove non-ASCII characters\n",
    "    text = ''.join(i for i in text if ord(i) < 128)\n",
    "    \n",
    "    # Step 8: Tokenize\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply the enhanced preprocessing function to each text entry in the DataFrame\n",
    "data['text'] = data['text'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0cfe940e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Only', 'if', 'they', 'try', 'to', 'take', 'the', 'moonshine', 'bottles', 'I', 'will', 'have', 'strapped', 'around', 'my', 'waste', 'for', 'race', 'hydration', 'running']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Only, if, they, try, to, take, the, moonshine...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[I, think, I, am, fully, rested, from, last, w...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[I, am, a, victim, A, victim, of, electrobitch...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[i, watched, your, TV, reality, show, thing, l...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[sorry, jen, we, can, maybe, watch, a, movie, ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>[Props, for, sure, Thanks, RE]</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>[sytycd, totally, did, not, think, they, would...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>[Stuck, in, ALDO, for, the, next, 6, hourswith...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>[raining, sunday]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>[a, sad, night, of, goodbyes]</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  target\n",
       "0      [Only, if, they, try, to, take, the, moonshine...       4\n",
       "1      [I, think, I, am, fully, rested, from, last, w...       4\n",
       "2      [I, am, a, victim, A, victim, of, electrobitch...       0\n",
       "3      [i, watched, your, TV, reality, show, thing, l...       4\n",
       "4      [sorry, jen, we, can, maybe, watch, a, movie, ...       0\n",
       "...                                                  ...     ...\n",
       "99995                     [Props, for, sure, Thanks, RE]       4\n",
       "99996  [sytycd, totally, did, not, think, they, would...       0\n",
       "99997  [Stuck, in, ALDO, for, the, next, 6, hourswith...       0\n",
       "99998                                  [raining, sunday]       0\n",
       "99999                      [a, sad, night, of, goodbyes]       0\n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data['text'][0])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "404e5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_and_lower(tokens):\n",
    "    cleaned_tokens = []\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        \n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        token = lemmatizer.lemmatize(token,pos)\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stopwords.words('english'):\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "data.loc[:,'text'] = data.loc[:,'text'].apply(lambda x: lemmatize_and_lower(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23ee64aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['try', 'take', 'moonshine', 'bottle', 'strap', 'around', 'waste', 'race', 'hydration', 'run']\n"
     ]
    }
   ],
   "source": [
    "print(data['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2eb314e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('tokenized.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15eaa780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        [try, take, moonshine, bottle, strap, around, ...\n",
      "1        [think, fully, rest, last, week, write, thanky...\n",
      "2        [victim, victim, electrobitching, mock, taste,...\n",
      "3        [watch, tv, reality, show, thing, last, night,...\n",
      "4        [sorry, jen, maybe, watch, movie, something, t...\n",
      "                               ...                        \n",
      "99995                                [props, sure, thanks]\n",
      "99996    [sytycd, totally, think, would, send, max, hom...\n",
      "99997    [stuck, aldo, next, 6, hourswith, die, phone, ...\n",
      "99998                                       [rain, sunday]\n",
      "99999                                [sad, night, goodbye]\n",
      "Name: text, Length: 100000, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc94d0ef",
   "metadata": {},
   "source": [
    "### Tf-Idf Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc49fc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import pos_tag\n",
    "import ast\n",
    "import nltk\n",
    "# nltk.download('averaged_perceptron_tagger') \n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6449808",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = pd.read_csv('tokenized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b696eef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a10d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten the list of lists into a single list of tokens\n",
    "all_tokens = []\n",
    "\n",
    "for token_string in tokenized['text']:\n",
    "    tokens = ast.literal_eval(token_string)\n",
    "    for token in tokens:\n",
    "        all_tokens.append(token)\n",
    "\n",
    "all_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0caaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the frequencies of each token\n",
    "token_frequencies = Counter(all_tokens)\n",
    "\n",
    "# Create a DataFrame from the token frequencies\n",
    "word_freq_df = pd.DataFrame(token_frequencies.items(), columns=['Word', 'Frequency']).sort_values(by='Frequency', ascending=False).reset_index(drop=True)\n",
    "\n",
    "word_freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f230df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Visualize the top 20 most common words\n",
    "top_n = 20\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.bar(word_freq_df['Word'][:top_n], word_freq_df['Frequency'][:top_n], color='skyblue')\n",
    "plt.xlabel('Words')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(f'Top {top_n} Most Common Words')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dec0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming word_freq_df is your DataFrame with the word frequencies\n",
    "\n",
    "# Histogram of word frequencies\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(word_freq_df['Frequency'], bins=50, color='skyblue', edgecolor='black')\n",
    "plt.title('Word Frequency Distribution')\n",
    "plt.xlabel('Word Frequency')\n",
    "plt.ylabel('Number of Words')\n",
    "plt.yscale('log')  # Use log scale for better visibility of lower frequencies\n",
    "plt.show()\n",
    "\n",
    "# Log-log plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(range(1, len(word_freq_df) + 1), word_freq_df['Frequency'].sort_values(ascending=False), marker='o', linestyle='-', color='skyblue')\n",
    "plt.title('Word Frequency Distribution (Log-Log Scale)')\n",
    "plt.xlabel('Rank of word (log scale)')\n",
    "plt.ylabel('Frequency of word (log scale)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140e065e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05c04a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: ' '.join(x))\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a269d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000,\n",
    "                             max_df=0.01, \n",
    "                             min_df=20, \n",
    "                             ngram_range=(1,2))\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584f2129",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names_out()\n",
    "tfidf_pd = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d56239",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca8b127",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pd.to_csv('tfidf_pd.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d6aa94",
   "metadata": {},
   "source": [
    "## PCA for dimentionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6046598",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_pd = pd.read_csv('tfidf_pd.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15659a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn import decomposition\n",
    "\n",
    "svd = decomposition.TruncatedSVD(n_components=200)\n",
    "svd_df = svd.fit_transform(tfidf_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013fcd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_df_pd = pd.DataFrame(svd_df, columns=[f'PC{i+1}' for i in range(svd_df.shape[1])])\n",
    "svd_df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa29d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_df_pd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac0c9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_df_pd.to_csv('svd_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff386eb",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc530d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X = pd.read_csv('svd_df.csv')\n",
    "y_ = pd.read_csv('tokenized.csv')['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae90fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert classes: 0 remains 0, and 4 becomes 1\n",
    "y = (y_ == 4).astype(int)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d9d1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68bf8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0414df0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dd743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_test,y))\n",
    "\n",
    "print(\"RMSE:\", rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b35f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the model\n",
    "model = LogisticRegression(solver='saga', max_iter=10000)\n",
    "\n",
    "# Define a grid of parameters to search over\n",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n",
    "\n",
    "# Setup the grid search\n",
    "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy', verbose=1)\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print(\"Best score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2c1d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = grid_search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f40874",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision = precision_score(y_test, y_)\n",
    "recall = recall_score(y_test, y_)\n",
    "\n",
    "precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf0e4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Create an interactive plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add Traces\n",
    "fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name='ROC Curve',\n",
    "                         line=dict(color='darkorange'),\n",
    "                         showlegend=True))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Chance',\n",
    "                         line=dict(color='navy', dash='dash'),\n",
    "                         showlegend=False))\n",
    "\n",
    "# Add AUC in the legend\n",
    "fig.update_layout(title=f'ROC Curve (AUC = {roc_auc:.2f})',\n",
    "                  xaxis_title='False Positive Rate',\n",
    "                  yaxis_title='True Positive Rate',\n",
    "                  xaxis=dict(showgrid=False),\n",
    "                  yaxis=dict(showgrid=False),\n",
    "                  template=\"plotly_white\")\n",
    "\n",
    "# Show figure\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1abc693",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f93b033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the logistic regression model using 'liblinear' solver\n",
    "model = LogisticRegression(solver='saga', random_state=42, max_iter=10000, tol=1e-4)\n",
    "\n",
    "# Define a grid of hyperparameter values to search over\n",
    "param_grid = {\n",
    "    'C': [ 0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2']  # liblinear supports both L1 and L2 regularization\n",
    "}\n",
    "\n",
    "# Define the AUC scoring function\n",
    "auc_scorer = make_scorer(roc_auc_score, greater_is_better=True, needs_proba=True)\n",
    "\n",
    "# Set up GridSearchCV\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=auc_scorer, cv=5, verbose=1)\n",
    "\n",
    "# Fit GridSearchCV to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# After fitting, you can check the best parameters and the best AUC score\n",
    "print(f\"Best parameters found: {grid_search.best_params_}\")\n",
    "print(f\"Best AUC score: {grid_search.best_score_}\")\n",
    "\n",
    "# Optionally, evaluate the best model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_ = best_model.predict_proba(X_test)[:, 1]  # Get probability estimates of the positive class\n",
    "test_auc = roc_auc_score(y_test, y_)\n",
    "\n",
    "print(f\"Test AUC score: {test_auc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d98002",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Compute ROC curve and AUC\n",
    "fpr, tpr, _ = roc_curve(y_test, y_)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Create an interactive plot\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add Traces\n",
    "fig.add_trace(go.Scatter(x=fpr, y=tpr, mode='lines', name='ROC Curve',\n",
    "                         line=dict(color='darkorange'),\n",
    "                         showlegend=True))\n",
    "\n",
    "fig.add_trace(go.Scatter(x=[0, 1], y=[0, 1], mode='lines', name='Chance',\n",
    "                         line=dict(color='navy', dash='dash'),\n",
    "                         showlegend=False))\n",
    "\n",
    "# Add AUC in the legend\n",
    "fig.update_layout(title=f'ROC Curve (AUC = {roc_auc:.2f})',\n",
    "                  xaxis_title='False Positive Rate',\n",
    "                  yaxis_title='True Positive Rate',\n",
    "                  xaxis=dict(showgrid=False),\n",
    "                  yaxis=dict(showgrid=False),\n",
    "                  template=\"plotly_white\")\n",
    "\n",
    "# Show figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0552e5bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd4c00b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
